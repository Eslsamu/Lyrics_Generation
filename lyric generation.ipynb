{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "6jL7ABuLx7CP",
    "outputId": "82b4b82c-4c89-48b0-8896-850895841465"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#!git clone https://github.com/davordavidovic/NLP-lyrics-generator.git\n",
    "  \n",
    "#!sudo pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WyZHGl9x7Cm"
   },
   "outputs": [],
   "source": [
    "def load_songs(genre, max_tokens):\n",
    "    df1 = pd.read_csv('data/lyrics_part1.csv')\n",
    "    df2 = pd.read_csv('data/lyrics_part2.csv')\n",
    "    df3 = pd.read_csv('data/lyrics_part3.csv')\n",
    "    df4 = pd.read_csv('data/lyrics_part4.csv')\n",
    "\n",
    "    df_part_1 = pd.concat([df1, df2])\n",
    "    df_part_2 = pd.concat([df3, df4])\n",
    "\n",
    "    df = pd.concat([df_part_1, df_part_2])\n",
    "    df.drop(columns=['index','Unnamed: 0'], inplace=True) #we dont need these columns\n",
    "\n",
    "    df = df.dropna() #there were around 10000 rows with no lyrics so drop them\n",
    "\n",
    "    df_songs = df[df.genre==genre]\n",
    "\n",
    "    df_songs['preprocessed'] = df_songs['lyrics'].map(prepare_text)\n",
    "\n",
    "    songs = df_songs.preprocessed.values\n",
    "\n",
    "    count = 0\n",
    "    cut = 0\n",
    "    for i,song in enumerate(songs):\n",
    "        tokens = song.split()\n",
    "        count += len(tokens) \n",
    "        if count >= max_tokens:\n",
    "            cut = i - 1\n",
    "        break\n",
    "    return songs[:cut]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjfON4R5x7DA"
   },
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', 'newline')\n",
    "  \n",
    "    text = text.split()\n",
    "  \n",
    "    for index, word in enumerate(text):\n",
    "        #remove non alphabetic characters at the end or beginning of a word\n",
    "        word = word.strip(string.punctuation)\n",
    "    \n",
    "        #replace non alhpanumeric chars with space\n",
    "        word = re.sub(r\"[\\W]\",' ',word)\n",
    "        text[index] = word \n",
    "   \n",
    "    #concatenate again\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqEpW5kjx7DM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_vocab(songs, min_frq):\n",
    "  #token pattern to also count one-character words\n",
    "  vectorizer = CountVectorizer(stop_words=[],min_df=min_frq,token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "  X = vectorizer.fit_transform(songs)\n",
    "\n",
    "\n",
    "  vocab_dict = vectorizer.vocabulary_\n",
    "  vocab_list =  list(vocab_dict)\n",
    "\n",
    "  return vocab_list, vocab_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNrFi8o-x7DU"
   },
   "outputs": [],
   "source": [
    "def index2sen(seq,vocab):\n",
    "    tokens = [vocab[int(t)] for t in seq]\n",
    "    sen = \" \".join(tokens)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciHlKPITx7DY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "def songs_to_supervised(seq_len, songs, vocab_dict, vocab_list):\n",
    "  data_x = []\n",
    "  data_y = []\n",
    "  seq_words = []\n",
    "\n",
    "  for song in songs:\n",
    "      tokens = song.split()\n",
    "      for i in range(0, len(tokens) - seq_len):\n",
    "          seq_in = tokens[i:i+seq_len]\n",
    "          seq_out = tokens[i + seq_len]\n",
    "          seq_data = []\n",
    "          \n",
    "          for word in seq_in:\n",
    "              if word in vocab_dict:\n",
    "                  seq_data.append(vocab_list.index(word))\n",
    "              else:\n",
    "                  break\n",
    "                  \n",
    "          #check if all words in sequence are in dict\n",
    "          if len(seq_data) == seq_len and seq_out in vocab_dict:\n",
    "              data_x.append(seq_data)\n",
    "              data_y.append(vocab_list.index(seq_out))\n",
    "              seq_words.append((seq_in,seq_out))\n",
    "              \n",
    "          '''\n",
    "          #return if enough sequences were created\n",
    "          if len(data_x) == n_seq:\n",
    "            return data_x, data_y \n",
    "          '''\n",
    "          \n",
    "  return data_x, data_y, seq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdUrn2U7x7D2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense,Dropout, CuDNNLSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "def create_model(layers, units, inp_shape, out_shape):\n",
    "  #lstm sequence to categoriemodel\n",
    "  model = Sequential()\n",
    "  \n",
    "  for l in range(layers-1):\n",
    "    model.add(CuDNNLSTM(units,return_sequences=True, input_shape = inp_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "  model.add(CuDNNLSTM(units,return_sequences=False))\n",
    "  model.add(Dropout(0.2)) \n",
    "  model.add(Dense(out_shape, activation='softmax'))\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHj1jiBY2GzQ"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, vocab_list):\n",
    "    n_vocab = len(vocab_list)\n",
    "    seq_in = prepare_text(seed_text).split()\n",
    "    x = np.array([vocab_list.index(word) for word in seq_in])/n_vocab\n",
    "    \n",
    "    output_word = \"\"\n",
    "    predictions = []\n",
    "    for i in range(next_words):\n",
    "        input_seq = np.reshape(np.append(x[i:],predictions),(1,len(x),1))\n",
    "        predicted = model.predict_classes(input_seq, verbose=0)\n",
    "        predictions.append(predicted[0])\n",
    "        output_word = vocab_list[predicted[0]]\n",
    "        seed_text += \" \" + output_word\n",
    "        #print(output_word, vocab_list[np.argmax(model.predict(input_seq,verbose=0))])\n",
    "        \n",
    "    return seed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ial_8bsflgmO"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    " \n",
    "  \n",
    "def run_experiment(n_sequences, n_epochs, genre, seq_len, n_layers, max_vocab_size, directory):\n",
    "\n",
    "  print(\"Running\", n_sequences,\"sequences\", n_epochs,\"epochs\",genre, seq_len,\"sequence length\", n_layers, \"layers\", max_vocab_size, \"vocab size\", directory, \"directory\") \n",
    "  \n",
    "  #load lyrics with this many tokens\n",
    "  max_tokens = n_sequences-seq_len\n",
    "  \n",
    "  #load song lyrics\n",
    "  songs = load_songs(genre, max_tokens)\n",
    "  \n",
    "  #create the right-sized vocabulary from the songs \n",
    "  min_frq = 1\n",
    "  n_vocab = np.inf\n",
    "  while n_vocab > max_vocab_size:\n",
    "    vocab_list, vocab_dict = build_vocab(songs, min_frq)\n",
    "    n_vocab = len(vocab_dict)\n",
    "    min_frq += 1\n",
    "  \n",
    "  #songs to sequences and labels\n",
    "  data_x, data_y, seq_words = songs_to_supervised(seq_len, songs, vocab_dict, vocab_list)\n",
    "  \n",
    "  #reshape input to samples, timesteps, features\n",
    "  X = np.reshape(data_x, (len(data_x), seq_len, 1))\n",
    "  #normalize input\n",
    "  X = X/float(n_vocab)\n",
    "  #categorical labels \n",
    "  y = np_utils.to_categorical(data_y)\n",
    "\n",
    "  inp_shape = X[0].shape\n",
    "  out_shape = y[0].shape[0]\n",
    "  print(\"X shape\",X.shape)\n",
    "  #create the lstm model\n",
    "  model = create_model(n_layers, units=400, inp_shape =inp_shape, out_shape=out_shape)\n",
    "  \n",
    "  # checkpoint\n",
    "  #TODO adapt filepath\n",
    "  filepath = directory + \"weights-improvement-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "  #early stopping \n",
    "  es = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=100)\n",
    "\n",
    "  callbacks_list = [es]\n",
    "  \n",
    "  #train model\n",
    "  history = model.fit(X, y, epochs=n_epochs, verbose=1,batch_size=1024,callbacks=callbacks_list, validation_split=0.1)\n",
    "  \n",
    "  #save model TODO namin\n",
    "  model.save(directory +\"model.h5\")\n",
    "  \n",
    "  #save history\n",
    "  with open(directory+\"hist\", 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "  \n",
    "  #generate validation texts and training texts\n",
    "  val_words = seq_words[:-10]\n",
    "  for t in val_words:\n",
    "    sentence = \" \".join(t[0])\n",
    "    label = t[1]\n",
    "    output = generate_text(sentence, next_words = seq_len, model = model, vocab_list = vocab_list)\n",
    "    with open(directory + \"generated.txt\",\"w\") as file:\n",
    "      file.write(sentence + \" out: \" + output + \"\\n\")\n",
    "      #also save the actual number of sequences that were used\n",
    "      file.write(str(len(data_x)))\n",
    "  \n",
    "  \n",
    "  #TODO save plot on training curve\n",
    "  plt.plot(history.history['acc'])\n",
    "  plt.title('model accuracy')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['training', 'test'], loc='upper left')\n",
    "  plot_path = directory + \"plot.png\"\n",
    "  plt.savefig(plot_path, bbox_inches='tight', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3892
    },
    "colab_type": "code",
    "id": "cyo8D4y_maAS",
    "outputId": "5addbfd8-f512-4ec9-84c7-c664a4cc3974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 13 experiments\n",
      "Running 500000 sequences 200 epochs Pop 20 sequence length 8 layers 300 vocab size ./gdrive/My Drive/Colab Notebooks/exps2/_300Pop directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f956f1f1b592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmax_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdir_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b516bc7f4fba>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(n_sequences, n_epochs, genre, seq_len, n_layers, max_vocab_size, directory)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m#load song lyrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0msongs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_songs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m#create the right-sized vocabulary from the songs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e69c4be32889>\u001b[0m in \u001b[0;36mload_songs\u001b[0;34m(genre, max_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/lyrics_part2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/lyrics_part3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/lyrics_part4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_part_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Uni/nlp/.env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Uni/nlp/.env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Uni/nlp/.env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Uni/nlp/.env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Uni/nlp/.env/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m     \"\"\"\n\u001b[1;32m   1704\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_sizes = [10000, 100000, 500000] #num of sequences\n",
    "epochs = [50,200]\n",
    "genres = ['Pop', 'Hip-Hop', 'Metal', 'Country']\n",
    "seq_lens = [5,20]\n",
    "layers = [4, 8] #400 units each\n",
    "max_vocab_size = [300, 800] #change min document frq until size fits\n",
    "\n",
    "experiments = []\n",
    "\n",
    "#big dataset on all genres with different vocabulary sizes\n",
    "for g in genres:\n",
    "  for m in max_vocab_size:\n",
    "    exp = {\"seqs\" : data_sizes[2],\n",
    "           \"epochs\" : epochs[1],\n",
    "           \"genre\" : g,\n",
    "           \"seq_lens\" : seq_lens[1],\n",
    "           \"layers\" : layers[1],\n",
    "           \"vocab\" : m,\n",
    "           \"dir\" : \"exps2/_\" + str(m) + g\n",
    "          }\n",
    "    experiments.append(exp)\n",
    "\n",
    "#different data sizes on hip hop\n",
    "for d in data_sizes:\n",
    "  exp = {\"seqs\" : d,\n",
    "         \"epochs\" : epochs[1],\n",
    "         \"genre\" : genres[1],\n",
    "         \"seq_lens\" : seq_lens[1],\n",
    "         \"layers\" : layers[1],\n",
    "         \"vocab\" : max_vocab_size[1],\n",
    "         \"dir\" : \"exps2/_\" + str(d) + \"sequences\"\n",
    "          }\n",
    "  experiments.append(exp)\n",
    "\n",
    "#different sequence lengths on hip hop\n",
    "for s in seq_lens:\n",
    "  exp = {\"seqs\" : data_sizes[2],\n",
    "         \"epochs\" : epochs[1],\n",
    "         \"genre\" : genres[1],\n",
    "         \"seq_lens\" : seq_lens[1],\n",
    "         \"layers\" : layers[1],\n",
    "         \"vocab\" : max_vocab_size[1],\n",
    "         \"dir\" : \"exps2/_\" + str(d) + \"sequences\"\n",
    "          }\n",
    "  experiments.append(exp)\n",
    "  \n",
    "        \n",
    "print(\"Running\", len(experiments), \"experiments\")\n",
    "            \n",
    "for e in experiments:\n",
    "  try:\n",
    "    n_seqs = e[\"seqs\"]\n",
    "    n_epochs = e[\"epochs\"]\n",
    "    genre = e[\"genre\"]\n",
    "    seq_len = e[\"seq_lens\"]\n",
    "    n_layers = e[\"layers\"]\n",
    "    max_vocab_size = e[\"vocab\"]\n",
    "    dir_ = e[\"dir\"]\n",
    "    run_experiment(n_sequences = n_seqs, n_epochs = n_epochs, genre = genre, seq_len = seq_len, n_layers = n_layers, max_vocab_size = max_vocab_size, directory = dir_)\n",
    "  except Exception as ex:\n",
    "    print(ex)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7ccRpjBUlM5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoh-hxHK2K2B"
   },
   "outputs": [],
   "source": [
    "#load from last checkpoint\n",
    "model.load_weights('content/gdrive/My Drive/Colab Notebooks/weights-improvement-77-0.41.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88Qw98N_1Ux3"
   },
   "outputs": [],
   "source": [
    "#train and save model\n",
    "history=model.fit(X, y, epochs=200, verbose=1,batch_size=1024,callbacks=callbacks_list, validation_split=0.1)\n",
    "model.save(\"./gdrive/My Drive/Colab Notebooks/200ep_4_lay_model_10000_pop_15seq.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEaxt9qZx7EK"
   },
   "outputs": [],
   "source": [
    "print(generate_text(\"Oh baby, baby, how was I supposed to know That something wasn't right here\",10,model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmM2QHV_bYph"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdjcTFwxt25-"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./gdrive/My Drive/Colab Notebooks/bigmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57CzlAFXx7Ek"
   },
   "outputs": [],
   "source": [
    "input_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sS6SGZDXuZQp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lyric generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
