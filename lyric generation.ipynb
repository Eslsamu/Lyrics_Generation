{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lyric generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jL7ABuLx7CP",
        "colab_type": "code",
        "outputId": "82b4b82c-4c89-48b0-8896-850895841465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!git clone https://github.com/davordavidovic/NLP-lyrics-generator.git\n",
        "  \n",
        "!sudo pip install h5py\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "fatal: destination path 'NLP-lyrics-generator' already exists and is not an empty directory.\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WyZHGl9x7Cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_songs(genre, max_tokens):\n",
        "  df1 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part1.csv')\n",
        "  df2 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part2.csv')\n",
        "  df3 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part3.csv')\n",
        "  df4 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part4.csv')\n",
        "\n",
        "  df_part_1 = pd.concat([df1, df2])\n",
        "  df_part_2 = pd.concat([df3, df4])\n",
        "\n",
        "  df = pd.concat([df_part_1, df_part_2])\n",
        "  df.drop(columns=['index','Unnamed: 0'], inplace=True) #we dont need these columns\n",
        "\n",
        "  df = df.dropna() #there were around 10000 rows with no lyrics so drop them\n",
        "  \n",
        "  df_songs = df[df.genre==genre]\n",
        "  \n",
        "  df_songs['preprocessed'] = df_songs['lyrics'].map(prepare_text)\n",
        "  \n",
        "  songs = df_songs.preprocessed.values\n",
        "  \n",
        "  count = 0\n",
        "  cut = 0\n",
        "  for i,song in enumerate(songs):\n",
        "      tokens = song.split()\n",
        "      count += len(tokens) \n",
        "      if count >= max_tokens:\n",
        "        cut = i - 1\n",
        "        break\n",
        "  return songs[:cut]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjfON4R5x7DA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', 'newline')\n",
        "  \n",
        "    text = text.split()\n",
        "  \n",
        "    for index, word in enumerate(text):\n",
        "        #remove non alphabetic characters at the end or beginning of a word\n",
        "        word = word.strip(string.punctuation)\n",
        "    \n",
        "        #replace non alhpanumeric chars with space\n",
        "        word = re.sub(r\"[\\W]\",' ',word)\n",
        "        text[index] = word \n",
        "   \n",
        "    #concatenate again\n",
        "    text = \" \".join(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqEpW5kjx7DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import OrderedDict\n",
        "\n",
        "def build_vocab(songs, min_frq):\n",
        "  #token pattern to also count one-character words\n",
        "  vectorizer = CountVectorizer(stop_words=[],min_df=min_frq,token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
        "  X = vectorizer.fit_transform(songs)\n",
        "\n",
        "\n",
        "  vocab_dict = vectorizer.vocabulary_\n",
        "  vocab_list =  list(vocab_dict)\n",
        "\n",
        "  return vocab_list, vocab_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNrFi8o-x7DU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index2sen(seq,vocab):\n",
        "    tokens = [vocab[int(t)] for t in seq]\n",
        "    sen = \" \".join(tokens)\n",
        "    return sen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciHlKPITx7DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "\n",
        "def songs_to_supervised(seq_len, songs, vocab_dict, vocab_list):\n",
        "  data_x = []\n",
        "  data_y = []\n",
        "  seq_words = []\n",
        "\n",
        "  for song in songs:\n",
        "      tokens = song.split()\n",
        "      for i in range(0, len(tokens) - seq_len):\n",
        "          seq_in = tokens[i:i+seq_len]\n",
        "          seq_out = tokens[i + seq_len]\n",
        "          seq_data = []\n",
        "          \n",
        "          for word in seq_in:\n",
        "              if word in vocab_dict:\n",
        "                  seq_data.append(vocab_list.index(word))\n",
        "              else:\n",
        "                  break\n",
        "                  \n",
        "          #check if all words in sequence are in dict\n",
        "          if len(seq_data) == seq_len and seq_out in vocab_dict:\n",
        "              data_x.append(seq_data)\n",
        "              data_y.append(vocab_list.index(seq_out))\n",
        "              seq_words.append((seq_in,seq_out))\n",
        "              \n",
        "          '''\n",
        "          #return if enough sequences were created\n",
        "          if len(data_x) == n_seq:\n",
        "            return data_x, data_y \n",
        "          '''\n",
        "          \n",
        "  return data_x, data_y, seq_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdUrn2U7x7D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense,Dropout, CuDNNLSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku \n",
        "\n",
        "def create_model(layers, units, inp_shape, out_shape):\n",
        "  #lstm sequence to categoriemodel\n",
        "  model = Sequential()\n",
        "  \n",
        "  for l in range(layers-1):\n",
        "    model.add(CuDNNLSTM(units,return_sequences=True, input_shape = inp_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "  model.add(CuDNNLSTM(units,return_sequences=False))\n",
        "  model.add(Dropout(0.2)) \n",
        "  model.add(Dense(out_shape, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHj1jiBY2GzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_text, next_words, model, vocab_list):\n",
        "    n_vocab = len(vocab_list)\n",
        "    seq_in = prepare_text(seed_text).split()\n",
        "    x = np.array([vocab_list.index(word) for word in seq_in])/n_vocab\n",
        "    \n",
        "    output_word = \"\"\n",
        "    predictions = []\n",
        "    for i in range(next_words):\n",
        "        input_seq = np.reshape(np.append(x[i:],predictions),(1,len(x),1))\n",
        "        predicted = model.predict_classes(input_seq, verbose=0)\n",
        "        predictions.append(predicted[0])\n",
        "        output_word = vocab_list[predicted[0]]\n",
        "        seed_text += \" \" + output_word\n",
        "        #print(output_word, vocab_list[np.argmax(model.predict(input_seq,verbose=0))])\n",
        "        \n",
        "    return seed_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ial_8bsflgmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "import numpy as np \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        " \n",
        "  \n",
        "def run_experiment(n_sequences, n_epochs, genre, seq_len, n_layers, max_vocab_size, directory):\n",
        "\n",
        "  print(\"Running\", n_sequences,\"sequences\", n_epochs,\"epochs\",genre, seq_len,\"sequence length\", n_layers, \"layers\", max_vocab_size, \"vocab size\", directory, \"directory\") \n",
        "  \n",
        "  #load lyrics with this many tokens\n",
        "  max_tokens = n_sequences-seq_len\n",
        "  \n",
        "  #load song lyrics\n",
        "  songs = load_songs(genre, max_tokens)\n",
        "  \n",
        "  #create the right-sized vocabulary from the songs \n",
        "  min_frq = 1\n",
        "  n_vocab = np.inf\n",
        "  while n_vocab > max_vocab_size:\n",
        "    vocab_list, vocab_dict = build_vocab(songs, min_frq)\n",
        "    n_vocab = len(vocab_dict)\n",
        "    min_frq += 1\n",
        "  \n",
        "  #songs to sequences and labels\n",
        "  data_x, data_y, seq_words = songs_to_supervised(seq_len, songs, vocab_dict, vocab_list)\n",
        "  \n",
        "  #reshape input to samples, timesteps, features\n",
        "  X = np.reshape(data_x, (len(data_x), seq_len, 1))\n",
        "  #normalize input\n",
        "  X = X/float(n_vocab)\n",
        "  #categorical labels \n",
        "  y = np_utils.to_categorical(data_y)\n",
        "\n",
        "  inp_shape = X[0].shape\n",
        "  out_shape = y[0].shape[0]\n",
        "  print(\"X shape\",X.shape)\n",
        "  #create the lstm model\n",
        "  model = create_model(n_layers, units=400, inp_shape =inp_shape, out_shape=out_shape)\n",
        "  \n",
        "  # checkpoint\n",
        "  #TODO adapt filepath\n",
        "  filepath = directory + \"weights-improvement-{epoch:02d}-{acc:.2f}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "  #early stopping \n",
        "  es = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=100)\n",
        "\n",
        "  callbacks_list = [es]\n",
        "  \n",
        "  #train model\n",
        "  history = model.fit(X, y, epochs=n_epochs, verbose=1,batch_size=1024,callbacks=callbacks_list, validation_split=0.1)\n",
        "  \n",
        "  #save model TODO namin\n",
        "  model.save(directory +\"model.h5\")\n",
        "  \n",
        "  #save history\n",
        "  with open(directory+\"hist\", 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)\n",
        "  \n",
        "  #generate validation texts and training texts\n",
        "  val_words = seq_words[:-10]\n",
        "  for t in val_words:\n",
        "    sentence = \" \".join(t[0])\n",
        "    label = t[1]\n",
        "    output = generate_text(sentence, next_words = seq_len, model = model, vocab_list = vocab_list)\n",
        "    with open(directory + \"generated.txt\",\"w\") as file:\n",
        "      file.write(sentence + \" out: \" + output + \"\\n\")\n",
        "      #also save the actual number of sequences that were used\n",
        "      file.write(str(len(data_x)))\n",
        "  \n",
        "  \n",
        "  #TODO save plot on training curve\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['training', 'test'], loc='upper left')\n",
        "  plot_path = directory + \"plot.png\"\n",
        "  plt.savefig(plot_path, bbox_inches='tight', format='png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyo8D4y_maAS",
        "colab_type": "code",
        "outputId": "5addbfd8-f512-4ec9-84c7-c664a4cc3974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3892
        }
      },
      "source": [
        "data_sizes = [10000, 100000, 500000] #num of sequences\n",
        "epochs = [50,200]\n",
        "genres = ['Pop', 'Hip-Hop', 'Metal', 'Country']\n",
        "seq_lens = [5,20]\n",
        "layers = [4, 8] #400 units each\n",
        "max_vocab_size = [300, 800] #change min document frq until size fits\n",
        "\n",
        "experiments = []\n",
        "\n",
        "#big dataset on all genres with different vocabulary sizes\n",
        "for g in genres:\n",
        "  for m in max_vocab_size:\n",
        "    exp = {\"seqs\" : data_sizes[2],\n",
        "           \"epochs\" : epochs[1],\n",
        "           \"genre\" : g,\n",
        "           \"seq_lens\" : seq_lens[1],\n",
        "           \"layers\" : layers[1],\n",
        "           \"vocab\" : m,\n",
        "           \"dir\" : \"./gdrive/My Drive/Colab Notebooks/exps2/_\" + str(m) + g\n",
        "          }\n",
        "    experiments.append(exp)\n",
        "\n",
        "#different data sizes on hip hop\n",
        "for d in data_sizes:\n",
        "  exp = {\"seqs\" : d,\n",
        "         \"epochs\" : epochs[1],\n",
        "         \"genre\" : genres[1],\n",
        "         \"seq_lens\" : seq_lens[1],\n",
        "         \"layers\" : layers[1],\n",
        "         \"vocab\" : max_vocab_size[1],\n",
        "         \"dir\" : \"./gdrive/My Drive/Colab Notebooks/exps2/_\" + str(d) + \"sequences\"\n",
        "          }\n",
        "  experiments.append(exp)\n",
        "\n",
        "#different sequence lengths on hip hop\n",
        "for s in seq_lens:\n",
        "  exp = {\"seqs\" : data_sizes[2],\n",
        "         \"epochs\" : epochs[1],\n",
        "         \"genre\" : genres[1],\n",
        "         \"seq_lens\" : seq_lens[1],\n",
        "         \"layers\" : layers[1],\n",
        "         \"vocab\" : max_vocab_size[1],\n",
        "         \"dir\" : \"./gdrive/My Drive/Colab Notebooks/exps2/_\" + str(d) + \"sequences\"\n",
        "          }\n",
        "  experiments.append(exp)\n",
        "  \n",
        "        \n",
        "print(\"Running\", len(experiments), \"experiments\")\n",
        "            \n",
        "for e in experiments:\n",
        "  try:\n",
        "    n_seqs = e[\"seqs\"]\n",
        "    n_epochs = e[\"epochs\"]\n",
        "    genre = e[\"genre\"]\n",
        "    seq_len = e[\"seq_lens\"]\n",
        "    n_layers = e[\"layers\"]\n",
        "    max_vocab_size = e[\"vocab\"]\n",
        "    dir_ = e[\"dir\"]\n",
        "    run_experiment(n_sequences = n_seqs, n_epochs = n_epochs, genre = genre, seq_len = seq_len, n_layers = n_layers, max_vocab_size = max_vocab_size, directory = dir_)\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 13 experiments\n",
            "Running 500000 sequences 200 epochs Pop 20 sequence length 8 layers 300 vocab size ./gdrive/My Drive/Colab Notebooks/exps2/_300Pop directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X shape (617, 20, 1)\n",
            "Train on 555 samples, validate on 62 samples\n",
            "Epoch 1/200\n",
            "555/555 [==============================] - 6s 11ms/step - loss: 5.6733 - acc: 0.0000e+00 - val_loss: 5.6644 - val_acc: 0.0000e+00\n",
            "Epoch 2/200\n",
            "555/555 [==============================] - 0s 873us/step - loss: 5.6570 - acc: 0.0631 - val_loss: 5.6213 - val_acc: 0.0000e+00\n",
            "Epoch 3/200\n",
            "555/555 [==============================] - 1s 905us/step - loss: 5.5720 - acc: 0.0631 - val_loss: 5.3712 - val_acc: 0.0000e+00\n",
            "Epoch 4/200\n",
            "555/555 [==============================] - 0s 896us/step - loss: 5.0474 - acc: 0.0559 - val_loss: 5.2602 - val_acc: 0.0000e+00\n",
            "Epoch 5/200\n",
            "555/555 [==============================] - 0s 889us/step - loss: 4.3980 - acc: 0.0541 - val_loss: 5.3357 - val_acc: 0.0645\n",
            "Epoch 6/200\n",
            "555/555 [==============================] - 0s 890us/step - loss: 4.2394 - acc: 0.0901 - val_loss: 5.3921 - val_acc: 0.0645\n",
            "Epoch 7/200\n",
            "555/555 [==============================] - 0s 893us/step - loss: 4.1325 - acc: 0.0631 - val_loss: 5.4865 - val_acc: 0.0000e+00\n",
            "Epoch 8/200\n",
            "555/555 [==============================] - 0s 896us/step - loss: 4.0740 - acc: 0.0667 - val_loss: 5.5560 - val_acc: 0.0000e+00\n",
            "Epoch 9/200\n",
            "555/555 [==============================] - 0s 897us/step - loss: 4.0706 - acc: 0.0468 - val_loss: 5.5745 - val_acc: 0.0645\n",
            "Epoch 10/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 4.0569 - acc: 0.0505 - val_loss: 5.5559 - val_acc: 0.0645\n",
            "Epoch 11/200\n",
            "555/555 [==============================] - 1s 912us/step - loss: 4.0107 - acc: 0.0595 - val_loss: 5.5350 - val_acc: 0.0645\n",
            "Epoch 12/200\n",
            "555/555 [==============================] - 0s 897us/step - loss: 3.9816 - acc: 0.0703 - val_loss: 5.5254 - val_acc: 0.0645\n",
            "Epoch 13/200\n",
            "555/555 [==============================] - 0s 895us/step - loss: 3.9826 - acc: 0.0631 - val_loss: 5.5231 - val_acc: 0.0645\n",
            "Epoch 14/200\n",
            "555/555 [==============================] - 0s 898us/step - loss: 3.9788 - acc: 0.0577 - val_loss: 5.5309 - val_acc: 0.0645\n",
            "Epoch 15/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 3.9463 - acc: 0.0541 - val_loss: 5.5438 - val_acc: 0.0645\n",
            "Epoch 16/200\n",
            "555/555 [==============================] - 1s 902us/step - loss: 3.9695 - acc: 0.0541 - val_loss: 5.5558 - val_acc: 0.0645\n",
            "Epoch 17/200\n",
            "555/555 [==============================] - 0s 896us/step - loss: 3.9559 - acc: 0.0541 - val_loss: 5.5695 - val_acc: 0.0645\n",
            "Epoch 18/200\n",
            "555/555 [==============================] - 0s 894us/step - loss: 3.9434 - acc: 0.0649 - val_loss: 5.5876 - val_acc: 0.0645\n",
            "Epoch 19/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.9508 - acc: 0.0559 - val_loss: 5.6126 - val_acc: 0.0645\n",
            "Epoch 20/200\n",
            "555/555 [==============================] - 0s 897us/step - loss: 3.9436 - acc: 0.0505 - val_loss: 5.6389 - val_acc: 0.0645\n",
            "Epoch 21/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 3.9519 - acc: 0.0703 - val_loss: 5.6624 - val_acc: 0.0645\n",
            "Epoch 22/200\n",
            "555/555 [==============================] - 0s 898us/step - loss: 3.9306 - acc: 0.0541 - val_loss: 5.6823 - val_acc: 0.0645\n",
            "Epoch 23/200\n",
            "555/555 [==============================] - 1s 905us/step - loss: 3.9467 - acc: 0.0649 - val_loss: 5.7019 - val_acc: 0.0645\n",
            "Epoch 24/200\n",
            "555/555 [==============================] - 0s 890us/step - loss: 3.9508 - acc: 0.0360 - val_loss: 5.7156 - val_acc: 0.0645\n",
            "Epoch 25/200\n",
            "555/555 [==============================] - 0s 894us/step - loss: 3.9659 - acc: 0.0450 - val_loss: 5.7237 - val_acc: 0.0645\n",
            "Epoch 26/200\n",
            "555/555 [==============================] - 1s 902us/step - loss: 3.9550 - acc: 0.0360 - val_loss: 5.7282 - val_acc: 0.0645\n",
            "Epoch 27/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.9457 - acc: 0.0739 - val_loss: 5.7312 - val_acc: 0.0645\n",
            "Epoch 28/200\n",
            "555/555 [==============================] - 0s 901us/step - loss: 3.9165 - acc: 0.0649 - val_loss: 5.7297 - val_acc: 0.0645\n",
            "Epoch 29/200\n",
            "555/555 [==============================] - 0s 895us/step - loss: 3.9493 - acc: 0.0577 - val_loss: 5.7298 - val_acc: 0.0645\n",
            "Epoch 30/200\n",
            "555/555 [==============================] - 0s 891us/step - loss: 3.9293 - acc: 0.0595 - val_loss: 5.7290 - val_acc: 0.0645\n",
            "Epoch 31/200\n",
            "555/555 [==============================] - 1s 905us/step - loss: 3.9230 - acc: 0.0559 - val_loss: 5.7277 - val_acc: 0.0645\n",
            "Epoch 32/200\n",
            "555/555 [==============================] - 1s 905us/step - loss: 3.9228 - acc: 0.0649 - val_loss: 5.7254 - val_acc: 0.0645\n",
            "Epoch 33/200\n",
            "555/555 [==============================] - 0s 890us/step - loss: 3.9273 - acc: 0.0685 - val_loss: 5.7238 - val_acc: 0.0645\n",
            "Epoch 34/200\n",
            "555/555 [==============================] - 1s 904us/step - loss: 3.9519 - acc: 0.0577 - val_loss: 5.7159 - val_acc: 0.0645\n",
            "Epoch 35/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.9327 - acc: 0.0721 - val_loss: 5.7070 - val_acc: 0.0645\n",
            "Epoch 36/200\n",
            "555/555 [==============================] - 1s 908us/step - loss: 3.9115 - acc: 0.0757 - val_loss: 5.6973 - val_acc: 0.0645\n",
            "Epoch 37/200\n",
            "555/555 [==============================] - 0s 897us/step - loss: 3.9353 - acc: 0.0541 - val_loss: 5.6864 - val_acc: 0.0645\n",
            "Epoch 38/200\n",
            "555/555 [==============================] - 0s 901us/step - loss: 3.9258 - acc: 0.0631 - val_loss: 5.6785 - val_acc: 0.0645\n",
            "Epoch 39/200\n",
            "555/555 [==============================] - 0s 893us/step - loss: 3.9113 - acc: 0.0865 - val_loss: 5.6741 - val_acc: 0.0645\n",
            "Epoch 40/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 3.9267 - acc: 0.0559 - val_loss: 5.6692 - val_acc: 0.0645\n",
            "Epoch 41/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 3.9111 - acc: 0.0595 - val_loss: 5.6665 - val_acc: 0.0645\n",
            "Epoch 42/200\n",
            "555/555 [==============================] - 0s 900us/step - loss: 3.9170 - acc: 0.0649 - val_loss: 5.6632 - val_acc: 0.0645\n",
            "Epoch 43/200\n",
            "555/555 [==============================] - 1s 906us/step - loss: 3.9054 - acc: 0.0631 - val_loss: 5.6606 - val_acc: 0.0645\n",
            "Epoch 44/200\n",
            "555/555 [==============================] - 1s 906us/step - loss: 3.9174 - acc: 0.0486 - val_loss: 5.6557 - val_acc: 0.0645\n",
            "Epoch 45/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 3.9111 - acc: 0.0541 - val_loss: 5.6494 - val_acc: 0.0645\n",
            "Epoch 46/200\n",
            "555/555 [==============================] - 1s 907us/step - loss: 3.9295 - acc: 0.0649 - val_loss: 5.6436 - val_acc: 0.0645\n",
            "Epoch 47/200\n",
            "555/555 [==============================] - 1s 904us/step - loss: 3.9167 - acc: 0.0577 - val_loss: 5.6376 - val_acc: 0.0645\n",
            "Epoch 48/200\n",
            "555/555 [==============================] - 0s 886us/step - loss: 3.9140 - acc: 0.0577 - val_loss: 5.6316 - val_acc: 0.0645\n",
            "Epoch 49/200\n",
            "555/555 [==============================] - 1s 902us/step - loss: 3.9405 - acc: 0.0595 - val_loss: 5.6284 - val_acc: 0.0645\n",
            "Epoch 50/200\n",
            "555/555 [==============================] - 0s 898us/step - loss: 3.9215 - acc: 0.0595 - val_loss: 5.6270 - val_acc: 0.0645\n",
            "Epoch 51/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.9260 - acc: 0.0613 - val_loss: 5.6251 - val_acc: 0.0645\n",
            "Epoch 52/200\n",
            "555/555 [==============================] - 1s 907us/step - loss: 3.9037 - acc: 0.0414 - val_loss: 5.6257 - val_acc: 0.0645\n",
            "Epoch 53/200\n",
            "555/555 [==============================] - 0s 892us/step - loss: 3.9140 - acc: 0.0541 - val_loss: 5.6262 - val_acc: 0.0645\n",
            "Epoch 54/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.8952 - acc: 0.0703 - val_loss: 5.6250 - val_acc: 0.0645\n",
            "Epoch 55/200\n",
            "555/555 [==============================] - 0s 896us/step - loss: 3.9142 - acc: 0.0559 - val_loss: 5.6199 - val_acc: 0.0645\n",
            "Epoch 56/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.9109 - acc: 0.0613 - val_loss: 5.6178 - val_acc: 0.0645\n",
            "Epoch 57/200\n",
            "555/555 [==============================] - 0s 892us/step - loss: 3.9196 - acc: 0.0523 - val_loss: 5.6167 - val_acc: 0.0645\n",
            "Epoch 58/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.9037 - acc: 0.0721 - val_loss: 5.6174 - val_acc: 0.0645\n",
            "Epoch 59/200\n",
            "555/555 [==============================] - 1s 910us/step - loss: 3.9326 - acc: 0.0523 - val_loss: 5.6182 - val_acc: 0.0645\n",
            "Epoch 60/200\n",
            "555/555 [==============================] - 0s 900us/step - loss: 3.9199 - acc: 0.0631 - val_loss: 5.6187 - val_acc: 0.0645\n",
            "Epoch 61/200\n",
            "555/555 [==============================] - 0s 893us/step - loss: 3.9038 - acc: 0.0775 - val_loss: 5.6194 - val_acc: 0.0645\n",
            "Epoch 62/200\n",
            "555/555 [==============================] - 1s 904us/step - loss: 3.9110 - acc: 0.0378 - val_loss: 5.6185 - val_acc: 0.0645\n",
            "Epoch 63/200\n",
            "555/555 [==============================] - 1s 907us/step - loss: 3.8979 - acc: 0.0721 - val_loss: 5.6173 - val_acc: 0.0645\n",
            "Epoch 64/200\n",
            "555/555 [==============================] - 1s 908us/step - loss: 3.9081 - acc: 0.0595 - val_loss: 5.6151 - val_acc: 0.0645\n",
            "Epoch 65/200\n",
            "555/555 [==============================] - 0s 896us/step - loss: 3.8996 - acc: 0.0559 - val_loss: 5.6101 - val_acc: 0.0645\n",
            "Epoch 66/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.9189 - acc: 0.0523 - val_loss: 5.6030 - val_acc: 0.0645\n",
            "Epoch 67/200\n",
            "555/555 [==============================] - 1s 912us/step - loss: 3.9024 - acc: 0.0757 - val_loss: 5.5968 - val_acc: 0.0645\n",
            "Epoch 68/200\n",
            "555/555 [==============================] - 1s 907us/step - loss: 3.8962 - acc: 0.0559 - val_loss: 5.5936 - val_acc: 0.0645\n",
            "Epoch 69/200\n",
            "555/555 [==============================] - 0s 897us/step - loss: 3.9075 - acc: 0.0577 - val_loss: 5.5951 - val_acc: 0.0645\n",
            "Epoch 70/200\n",
            "555/555 [==============================] - 1s 906us/step - loss: 3.9102 - acc: 0.0613 - val_loss: 5.5992 - val_acc: 0.0645\n",
            "Epoch 71/200\n",
            "555/555 [==============================] - 1s 910us/step - loss: 3.8971 - acc: 0.0523 - val_loss: 5.6063 - val_acc: 0.0645\n",
            "Epoch 72/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.8979 - acc: 0.0757 - val_loss: 5.6129 - val_acc: 0.0645\n",
            "Epoch 73/200\n",
            "555/555 [==============================] - 0s 896us/step - loss: 3.9066 - acc: 0.0414 - val_loss: 5.6165 - val_acc: 0.0645\n",
            "Epoch 74/200\n",
            "555/555 [==============================] - 1s 907us/step - loss: 3.9033 - acc: 0.0486 - val_loss: 5.6182 - val_acc: 0.0645\n",
            "Epoch 75/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.8946 - acc: 0.0486 - val_loss: 5.6190 - val_acc: 0.0645\n",
            "Epoch 76/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.9068 - acc: 0.0703 - val_loss: 5.6199 - val_acc: 0.0645\n",
            "Epoch 77/200\n",
            "555/555 [==============================] - 0s 901us/step - loss: 3.9050 - acc: 0.0721 - val_loss: 5.6238 - val_acc: 0.0645\n",
            "Epoch 78/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.8936 - acc: 0.0667 - val_loss: 5.6315 - val_acc: 0.0645\n",
            "Epoch 79/200\n",
            "555/555 [==============================] - 1s 904us/step - loss: 3.8936 - acc: 0.0667 - val_loss: 5.6405 - val_acc: 0.0645\n",
            "Epoch 80/200\n",
            "555/555 [==============================] - 1s 910us/step - loss: 3.8958 - acc: 0.0739 - val_loss: 5.6499 - val_acc: 0.0645\n",
            "Epoch 81/200\n",
            "555/555 [==============================] - 1s 901us/step - loss: 3.8986 - acc: 0.0595 - val_loss: 5.6570 - val_acc: 0.0645\n",
            "Epoch 82/200\n",
            "555/555 [==============================] - 1s 907us/step - loss: 3.9016 - acc: 0.0613 - val_loss: 5.6590 - val_acc: 0.0645\n",
            "Epoch 83/200\n",
            "555/555 [==============================] - 0s 901us/step - loss: 3.8947 - acc: 0.0721 - val_loss: 5.6563 - val_acc: 0.0645\n",
            "Epoch 84/200\n",
            "555/555 [==============================] - 0s 900us/step - loss: 3.9135 - acc: 0.0559 - val_loss: 5.6523 - val_acc: 0.0645\n",
            "Epoch 85/200\n",
            "555/555 [==============================] - 1s 903us/step - loss: 3.8941 - acc: 0.0613 - val_loss: 5.6476 - val_acc: 0.0645\n",
            "Epoch 86/200\n",
            "555/555 [==============================] - 0s 893us/step - loss: 3.8883 - acc: 0.0703 - val_loss: 5.6435 - val_acc: 0.0645\n",
            "Epoch 87/200\n",
            "555/555 [==============================] - 1s 902us/step - loss: 3.9008 - acc: 0.0468 - val_loss: 5.6382 - val_acc: 0.0645\n",
            "Epoch 88/200\n",
            "555/555 [==============================] - 0s 894us/step - loss: 3.9161 - acc: 0.0757 - val_loss: 5.6340 - val_acc: 0.0645\n",
            "Epoch 89/200\n",
            "555/555 [==============================] - 1s 912us/step - loss: 3.8952 - acc: 0.0847 - val_loss: 5.6306 - val_acc: 0.0645\n",
            "Epoch 90/200\n",
            "555/555 [==============================] - 1s 906us/step - loss: 3.9051 - acc: 0.0667 - val_loss: 5.6279 - val_acc: 0.0645\n",
            "Epoch 91/200\n",
            "555/555 [==============================] - 1s 908us/step - loss: 3.9054 - acc: 0.0649 - val_loss: 5.6251 - val_acc: 0.0645\n",
            "Epoch 92/200\n",
            "555/555 [==============================] - 1s 922us/step - loss: 3.8953 - acc: 0.0685 - val_loss: 5.6247 - val_acc: 0.0645\n",
            "Epoch 93/200\n",
            "555/555 [==============================] - 1s 911us/step - loss: 3.8998 - acc: 0.0631 - val_loss: 5.6241 - val_acc: 0.0645\n",
            "Epoch 94/200\n",
            "555/555 [==============================] - 1s 915us/step - loss: 3.8896 - acc: 0.0523 - val_loss: 5.6228 - val_acc: 0.0645\n",
            "Epoch 95/200\n",
            "555/555 [==============================] - 1s 904us/step - loss: 3.8929 - acc: 0.0613 - val_loss: 5.6231 - val_acc: 0.0645\n",
            "Epoch 96/200\n",
            "555/555 [==============================] - 1s 912us/step - loss: 3.9002 - acc: 0.0559 - val_loss: 5.6238 - val_acc: 0.0645\n",
            "Epoch 97/200\n",
            "555/555 [==============================] - 1s 910us/step - loss: 3.8980 - acc: 0.0613 - val_loss: 5.6260 - val_acc: 0.0645\n",
            "Epoch 98/200\n",
            "555/555 [==============================] - 1s 912us/step - loss: 3.9057 - acc: 0.0667 - val_loss: 5.6271 - val_acc: 0.0645\n",
            "Epoch 99/200\n",
            "555/555 [==============================] - 1s 916us/step - loss: 3.9086 - acc: 0.0613 - val_loss: 5.6302 - val_acc: 0.0645\n",
            "Epoch 100/200\n",
            "555/555 [==============================] - 0s 898us/step - loss: 3.9005 - acc: 0.0559 - val_loss: 5.6343 - val_acc: 0.0645\n",
            "Epoch 101/200\n",
            "555/555 [==============================] - 0s 899us/step - loss: 3.9084 - acc: 0.0685 - val_loss: 5.6357 - val_acc: 0.0645\n",
            "Epoch 00101: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7ccRpjBUlM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoh-hxHK2K2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load from last checkpoint\n",
        "model.load_weights('content/gdrive/My Drive/Colab Notebooks/weights-improvement-77-0.41.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88Qw98N_1Ux3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train and save model\n",
        "history=model.fit(X, y, epochs=200, verbose=1,batch_size=1024,callbacks=callbacks_list, validation_split=0.1)\n",
        "model.save(\"./gdrive/My Drive/Colab Notebooks/200ep_4_lay_model_10000_pop_15seq.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEaxt9qZx7EK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generate_text(\"Oh baby, baby, how was I supposed to know That something wasn't right here\",10,model))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmM2QHV_bYph",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdjcTFwxt25-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('./gdrive/My Drive/Colab Notebooks/bigmodel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57CzlAFXx7Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS6SGZDXuZQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}