{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#!git clone https://github.com/davordavidovic/NLP-lyrics-generator.git\n",
    "  \n",
    "#!sudo pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(genre, max_tokens):\n",
    "    df1 = pd.read_csv('data/lyrics_part1.csv')\n",
    "    df2 = pd.read_csv('data/lyrics_part2.csv')\n",
    "    df3 = pd.read_csv('data/lyrics_part3.csv')\n",
    "    df4 = pd.read_csv('data/lyrics_part4.csv')\n",
    "\n",
    "    df_part_1 = pd.concat([df1, df2])\n",
    "    df_part_2 = pd.concat([df3, df4])\n",
    "\n",
    "    df = pd.concat([df_part_1, df_part_2])\n",
    "    df.drop(columns=['index','Unnamed: 0'], inplace=True) #we dont need these columns\n",
    "\n",
    "    df = df.dropna() #there were around 10000 rows with no lyrics so drop them\n",
    "\n",
    "    df_songs = df[df.genre==genre]\n",
    "\n",
    "    df_songs['preprocessed'] = df_songs['lyrics'].map(prepare_text)\n",
    "\n",
    "    songs = df_songs.preprocessed.values\n",
    "    \n",
    "    count = 0\n",
    "    cut = 0\n",
    "    for i,song in enumerate(songs):\n",
    "        tokens = list(song)\n",
    "        count += len(tokens) \n",
    "        if count >= max_tokens:\n",
    "            cut = i - 1\n",
    "            break\n",
    "    \n",
    "    return songs[:cut]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' N ')\n",
    "  \n",
    "    text = text.split()\n",
    "\n",
    "    for index, word in enumerate(text):\n",
    "        #remove non alphabetic characters at the end or beginning of a word\n",
    "        word = word.strip(string.punctuation)\n",
    "    \n",
    "        #replace non alhpanumeric chars with space\n",
    "        word = re.sub(r\"[\\W]\",' ',word)\n",
    "        text[index] = word \n",
    "   \n",
    "    #concatenate again\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(songs):\n",
    "    # create mapping of unique chars to integers\n",
    "    chars = sorted(list(set(\" \".join(songs))))\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "    return chars, char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index2sen(seq,chars):\n",
    "    tokens = [chars[int(t)] for t in seq]\n",
    "    sen = \"\".join(tokens)\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def songs_to_supervised(seq_len, songs, char_to_int):\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "\n",
    "    for song in songs:\n",
    "        tokens = list(song)\n",
    "        for i in range(0, len(tokens) - seq_len):\n",
    "            seq_in = tokens[i:i+seq_len]\n",
    "            seq_out = tokens[i + seq_len]\n",
    "            seq_data = []\n",
    "            for c in seq_in:\n",
    "                data_x.append(char_to_int[c])\n",
    "\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense,Dropout, CuDNNLSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "def create_model(layers, units, inp_shape, out_shape):\n",
    "    #lstm sequence to categoriemodel\n",
    "    model = Sequential()\n",
    "  \n",
    "    for l in range(layers-1):\n",
    "        model.add(CuDNNLSTM(units,return_sequences=True, input_shape = inp_shape))\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(CuDNNLSTM(units,return_sequences=False))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(Dense(out_shape, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_chars, model, chars, chars_to_int):\n",
    "    seq_in = list(prepare_text(seed_text))\n",
    "    x = np.array([chars_to_int[c] for c in seq_in])\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(next_words):\n",
    "        input_seq = np.reshape(np.append(x[i:],predictions),(1,len(x),1))\n",
    "        predicted = model.predict_classes(input_seq, verbose=0)\n",
    "        predictions.append(predicted[0])\n",
    "        output_word = chars[predicted[0]]\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "    return seed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    " \n",
    "\n",
    "def run_experiment(n_sequences, n_epochs, genre, seq_len, n_layers, directory):\n",
    "\n",
    "    print(\"Running\", n_sequences,\"sequences\", n_epochs,\"epochs\",genre, seq_len,\"sequence length\", n_layers, \"layers\", \"vocab size\", directory, \"directory\") \n",
    "\n",
    "    #load lyrics with this many tokens\n",
    "    max_tokens = n_sequences-seq_len\n",
    "\n",
    "    #load song lyrics\n",
    "    songs = load_songs(genre, max_tokens)\n",
    "\n",
    "    #create the vocabulary from the songs \n",
    "    chars, chars_to_int = build_vocab(songs)\n",
    "    n_vocab = len(chars)\n",
    "    #songs to sequences and labels\n",
    "    data_x, data_y = songs_to_supervised(seq_len, songs, chars_to_int)\n",
    "    \n",
    "    \n",
    "    #reshape input to samples, timesteps, features\n",
    "    X = np.reshape(data_x, (len(data_x), seq_len, 1))\n",
    "    #normalize input\n",
    "    X = X/float(n_vocab)\n",
    "    #categorical labels \n",
    "    y = np_utils.to_categorical(data_y)\n",
    "\n",
    "    inp_shape = X[0].shape\n",
    "    out_shape = y[0].shape[0]\n",
    "    print(\"X shape\",X.shape)\n",
    "    #create the lstm model\n",
    "    model = create_model(n_layers, units=400, inp_shape =inp_shape, out_shape=out_shape)\n",
    "\n",
    "    # checkpoint\n",
    "    #TODO adapt filepath\n",
    "    filepath = directory + \"weights-improvement-{epoch:02d}-{acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    #early stopping \n",
    "    es = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=100)\n",
    "\n",
    "    callbacks_list = [es]\n",
    "\n",
    "    #train model\n",
    "    history = model.fit(X, y, epochs=n_epochs, verbose=1,batch_size=1024,callbacks=callbacks_list, validation_split=0.1)\n",
    "\n",
    "    #save model TODO namin\n",
    "    model.save(directory +\"model.h5\")\n",
    "\n",
    "    #save history\n",
    "    with open(directory+\"hist\", 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    #generate validation texts and training texts\n",
    "    val_words = seq_words[:-10]\n",
    "    for t in val_words:\n",
    "        sentence = \" \".join(t[0])\n",
    "        label = t[1]\n",
    "        output = generate_text(sentence, next_words = seq_len, model = model, vocab_list = vocab_list)\n",
    "        with open(directory + \"generated.txt\",\"w\") as file:\n",
    "            file.write(sentence + \" out: \" + output + \"\\n\")\n",
    "            #also save the actual number of sequences that were used\n",
    "            file.write(str(len(data_x)))\n",
    "  \n",
    "  \n",
    "    #TODO save plot on training curve\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'test'], loc='upper left')\n",
    "    plot_path = directory + \"plot.png\"\n",
    "    plt.savefig(plot_path, bbox_inches='tight', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sizes = [10000, 100000, 5000000] #num of sequences\n",
    "epochs = [50,200]\n",
    "genres = ['Pop', 'Hip-Hop', 'Metal', 'Country']\n",
    "seq_lens = [20,50]\n",
    "layers = [4, 8] #400 units each\n",
    "\n",
    "experiments = []\n",
    "\n",
    "#big dataset on all genres with different vocabulary sizes\n",
    "for g in genres:\n",
    "    exp = {\"seqs\" : data_sizes[2],\n",
    "           \"epochs\" : epochs[1],\n",
    "           \"genre\" : g,\n",
    "           \"seq_lens\" : seq_lens[1],\n",
    "           \"layers\" : layers[0],\n",
    "           \"dir\" : \"exps2/_\" +  g\n",
    "          }\n",
    "    experiments.append(exp)\n",
    "\n",
    "print(\"Running\", len(experiments), \"experiments\")\n",
    "            \n",
    "for e in experiments:\n",
    "    #try:\n",
    "        n_seqs = e[\"seqs\"]\n",
    "        n_epochs = e[\"epochs\"]\n",
    "        genre = e[\"genre\"]\n",
    "        seq_len = e[\"seq_lens\"]\n",
    "        n_layers = e[\"layers\"]\n",
    "        dir_ = e[\"dir\"]\n",
    "        run_experiment(n_sequences = n_seqs, n_epochs = n_epochs, genre = genre, seq_len = seq_len, n_layers = n_layers, directory = dir_)\n",
    "    #except Exception as ex:\n",
    "     #   print(ex)\n",
    "      #  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
