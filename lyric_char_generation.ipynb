{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lyric_char_generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8SdgVTt4Ljw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "802ca705-4f6c-49c3-9a1f-e503bef63f8b"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!git clone https://github.com/davordavidovic/NLP-lyrics-generator.git\n",
        "  \n",
        "#!sudo pip install h5py\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Cloning into 'NLP-lyrics-generator'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 22 (delta 0), reused 22 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg7fBagm4Lj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_songs(genre, max_tokens):\n",
        "    df1 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part1.csv')\n",
        "    df2 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part2.csv')\n",
        "    df3 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part3.csv')\n",
        "    df4 = pd.read_csv('./NLP-lyrics-generator/data/lyrics_part4.csv')\n",
        "\n",
        "    df_part_1 = pd.concat([df1, df2])\n",
        "    df_part_2 = pd.concat([df3, df4])\n",
        "\n",
        "    df = pd.concat([df_part_1, df_part_2])\n",
        "    df.drop(columns=['index','Unnamed: 0'], inplace=True) #we dont need these columns\n",
        "\n",
        "    df = df.dropna() #there were around 10000 rows with no lyrics so drop them\n",
        "\n",
        "    df_songs = df[df.genre==genre]\n",
        "\n",
        "    df_songs['preprocessed'] = df_songs['lyrics'].map(prepare_text)\n",
        "\n",
        "    songs = df_songs.preprocessed.values\n",
        "    \n",
        "    count = 0\n",
        "    cut = 0\n",
        "    for i,song in enumerate(songs):\n",
        "        tokens = list(song)\n",
        "        count += len(tokens) \n",
        "        if count >= max_tokens:\n",
        "            cut = i - 1\n",
        "            break\n",
        "    \n",
        "    return songs[:cut]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiT3m3kP4Lj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' N ')\n",
        "  \n",
        "    text = text.split()\n",
        "\n",
        "    for index, word in enumerate(text):\n",
        "        #remove non alphabetic characters at the end or beginning of a word\n",
        "        #word = word.strip(string.punctuation)\n",
        "    \n",
        "        #replace non alhpanumeric chars with space\n",
        "        word = re.sub(r\"[\\W]\",'',word)\n",
        "        text[index] = word \n",
        "   \n",
        "    #concatenate again\n",
        "    text = \" \".join(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8bbBGhi4Lj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(songs):\n",
        "    # create mapping of unique chars to integers\n",
        "    chars = sorted(list(set(\" \".join(songs))))\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "    return chars, char_to_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZByMdMeJ4LkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index2sen(seq,chars):\n",
        "    tokens = [chars[int(t)] for t in seq]\n",
        "    sen = \"\".join(tokens)\n",
        "    return sen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r6MG_9R4LkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def songs_to_supervised(seq_len, songs, char_to_int):\n",
        "    data_x = []\n",
        "    data_y = []\n",
        "    seq_chars = []\n",
        "    \n",
        "    for song in songs:\n",
        "        tokens = list(song)\n",
        "        for i in range(0, len(tokens) - seq_len):\n",
        "            seq_in = tokens[i:i+seq_len]\n",
        "            seq_out = tokens[i + seq_len]\n",
        "            data_x.append([char_to_int[c] for c in seq_in])\n",
        "            data_y.append(char_to_int[seq_out])\n",
        "            seq_chars.append((seq_in,seq_out))\n",
        "\n",
        "    return data_x, data_y, seq_chars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkWVUbBk4LkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ae2d71d-6a3a-4200-a872-9eb4ab564c13"
      },
      "source": [
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense,Dropout, CuDNNLSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku \n",
        "\n",
        "def create_model(layers, units, inp_shape, out_shape):\n",
        "    #lstm sequence to categoriemodel\n",
        "    model = Sequential()\n",
        "  \n",
        "    for l in range(layers-1):\n",
        "        model.add(CuDNNLSTM(units,return_sequences=True, input_shape = inp_shape))\n",
        "        model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(CuDNNLSTM(units,return_sequences=False))\n",
        "    model.add(Dropout(0.2)) \n",
        "    model.add(Dense(out_shape, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['categorical_accuracy'])\n",
        "  \n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e9GC6964LkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed_text, next_chars, model, chars, chars_to_int):\n",
        "    seq_in = list(seed_text)\n",
        "    x = np.array([chars_to_int[c] for c in seq_in])\n",
        "    predictions = []\n",
        "    for i in range(next_chars):\n",
        "        input_seq = np.reshape(np.append(x[i:],predictions),(1,len(x),1))\n",
        "        predicted = model.predict_classes(input_seq, verbose=0)\n",
        "        predictions.append(predicted[0])\n",
        "        output_char = chars[predicted[0]]\n",
        "        seed_text += \" \" + output_char\n",
        "        \n",
        "    return seed_text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU2wSWQd4LkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils\n",
        "import numpy as np \n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        " \n",
        "\n",
        "def run_experiment(n_sequences, n_epochs, genre, seq_len, n_layers, directory):\n",
        "\n",
        "    print(\"Running\", n_sequences,\"sequences\", n_epochs,\"epochs\",genre, seq_len,\"sequence length\", n_layers, \"layers\", \"vocab size\", directory, \"directory\") \n",
        "\n",
        "    #load lyrics with this many tokens\n",
        "    max_tokens = n_sequences-seq_len\n",
        "\n",
        "    #load song lyrics\n",
        "    songs = load_songs(genre, max_tokens)\n",
        "\n",
        "    #create the vocabulary from the songs \n",
        "    chars, chars_to_int = build_vocab(songs)\n",
        "    n_vocab = len(chars)\n",
        "    #songs to sequences and labels\n",
        "    data_x, data_y, seq_chars = songs_to_supervised(seq_len, songs, chars_to_int)\n",
        "    \n",
        "    #reshape input to samples, timesteps, features\n",
        "    X = np.reshape(data_x, (len(data_x), seq_len, 1))\n",
        "    #normalize input\n",
        "    X = X/float(n_vocab)\n",
        "    #categorical labels \n",
        "    y = np_utils.to_categorical(data_y)\n",
        "\n",
        "    inp_shape = X[0].shape\n",
        "    out_shape = y[0].shape[0]\n",
        "    print(\"X shape\",X.shape)\n",
        "    #create the lstm model\n",
        "    model = create_model(n_layers, units=400, inp_shape =inp_shape, out_shape=out_shape)\n",
        "\n",
        "    # checkpoint\n",
        "    #TODO adapt filepath\n",
        "    filepath = directory + \"weights-improvement-{epoch:02d}-{acc:.2f}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "    #early stopping \n",
        "    es = EarlyStopping(monitor='val_acc', mode='min', verbose=1, patience=50)\n",
        "\n",
        "    callbacks_list = [es]\n",
        "\n",
        "    #train model\n",
        "    history = model.fit(X, y, epochs=n_epochs, verbose=1,batch_size=1024,callbacks=callbacks_list, validation_split=0.1)\n",
        "\n",
        "    #save model TODO namin\n",
        "    model.save(directory +\"model.h5\")\n",
        "\n",
        "    #save history\n",
        "    with open(directory+\"hist\", 'wb') as file_pi:\n",
        "        pickle.dump(history.history, file_pi)\n",
        "\n",
        "    #generate validation texts and training texts\n",
        "    val_chars = seq_chars[-5:]\n",
        "    for t in val_chars:\n",
        "        sentence = \"\".join(t[0])\n",
        "        label = t[1]\n",
        "        output = generate_text(sentence, next_chars = seq_len, model = model, chars=chars, chars_to_int=chars_to_int)\n",
        "        with open(directory + \"generated.txt\",\"w\") as file:\n",
        "            print(sentence + \" out: \" + output)\n",
        "            file.write(sentence + \" out: \" + output + \"\\n\")\n",
        "            #also save the actual number of sequences that were used\n",
        "            file.write(str(len(data_x)))\n",
        "  \n",
        "  \n",
        "    #TODO save plot on training curve\n",
        "    plt.plot(history.history['categorical_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['training', 'test'], loc='upper left')\n",
        "    plot_path = directory + \"plot.png\"\n",
        "    plt.savefig(plot_path, bbox_inches='tight', format='png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDlh-uni4Lkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8534
        },
        "outputId": "e80ef621-79d1-49f5-bb5f-1721b7542001"
      },
      "source": [
        "data_sizes = [10000, 100000, 250000] #num of sequences\n",
        "epochs = [5,70]\n",
        "genres = ['Pop', 'Hip-Hop', 'Metal', 'Country']\n",
        "seq_lens = [5,20,30]\n",
        "layers = [4, 8] #400 units each\n",
        "\n",
        "experiments = []\n",
        " \n",
        "#big dataset on all genres with different vocabulary sizes\n",
        "for s in seq_lens:\n",
        "    exp = {\"seqs\" : data_sizes[1],\n",
        "           \"epochs\" : epochs[1],\n",
        "           \"genre\" : genres[1],\n",
        "           \"seq_lens\" : s,\n",
        "           \"layers\" : layers[0],\n",
        "           \"dir\" : \"./gdrive/My Drive/Colab Notebooks/exps3/_slen\" +  str(s)\n",
        "          }\n",
        "    experiments.append(exp)\n",
        "'''\n",
        "#big dataset on all genres with different vocabulary sizes\n",
        "for g in genres:\n",
        "    exp = {\"seqs\" : data_sizes[2],\n",
        "           \"epochs\" : epochs[1],\n",
        "           \"genre\" : g,\n",
        "           \"seq_lens\" : seq_lens[2],\n",
        "           \"layers\" : layers[0],\n",
        "           \"dir\" : \"exps3/_250000_100e_30seq_\" +  g\n",
        "          }\n",
        "    experiments.append(exp)\n",
        "'''  \n",
        "print(\"Running\", len(experiments), \"experiments\")\n",
        "            \n",
        "for e in experiments:\n",
        "    #try:\n",
        "        n_seqs = e[\"seqs\"]\n",
        "        n_epochs = e[\"epochs\"]\n",
        "        genre = e[\"genre\"]\n",
        "        seq_len = e[\"seq_lens\"]\n",
        "        n_layers = e[\"layers\"]\n",
        "        dir_ = e[\"dir\"]\n",
        "        run_experiment(n_sequences = n_seqs, n_epochs = n_epochs, genre = genre, seq_len = seq_len, n_layers = n_layers, directory = dir_)\n",
        "    #except Exception as ex:\n",
        "     #   print(ex)\n",
        "      #  pass"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 3 experiments\n",
            "Running 100000 sequences 70 epochs Hip-Hop 5 sequence length 4 layers vocab size ./gdrive/My Drive/Colab Notebooks/exps3/_slen5 directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X shape (97378, 5, 1)\n",
            "Train on 87640 samples, validate on 9738 samples\n",
            "Epoch 1/70\n",
            "87640/87640 [==============================] - 6s 68us/step - loss: 2.9593 - categorical_accuracy: 0.2180 - val_loss: 2.8881 - val_categorical_accuracy: 0.2295\n",
            "Epoch 2/70\n",
            " 3072/87640 [>.............................] - ETA: 4s - loss: 2.8925 - categorical_accuracy: 0.2168"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_categorical_accuracy,loss,categorical_accuracy\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "87640/87640 [==============================] - 5s 55us/step - loss: 2.8900 - categorical_accuracy: 0.2207 - val_loss: 2.8900 - val_categorical_accuracy: 0.2295\n",
            "Epoch 3/70\n",
            "87640/87640 [==============================] - 5s 55us/step - loss: 2.8880 - categorical_accuracy: 0.2207 - val_loss: 2.8895 - val_categorical_accuracy: 0.2295\n",
            "Epoch 4/70\n",
            "87640/87640 [==============================] - 5s 56us/step - loss: 2.8755 - categorical_accuracy: 0.2207 - val_loss: 2.8452 - val_categorical_accuracy: 0.2276\n",
            "Epoch 5/70\n",
            "87640/87640 [==============================] - 5s 56us/step - loss: 2.7175 - categorical_accuracy: 0.2413 - val_loss: 2.5924 - val_categorical_accuracy: 0.2753\n",
            "Epoch 6/70\n",
            "87640/87640 [==============================] - 5s 56us/step - loss: 2.5244 - categorical_accuracy: 0.2827 - val_loss: 2.4246 - val_categorical_accuracy: 0.3086\n",
            "Epoch 7/70\n",
            "87640/87640 [==============================] - 5s 56us/step - loss: 2.3555 - categorical_accuracy: 0.3191 - val_loss: 2.2393 - val_categorical_accuracy: 0.3517\n",
            "Epoch 8/70\n",
            "87640/87640 [==============================] - 5s 57us/step - loss: 2.2054 - categorical_accuracy: 0.3565 - val_loss: 2.1250 - val_categorical_accuracy: 0.3933\n",
            "Epoch 9/70\n",
            "87640/87640 [==============================] - 5s 57us/step - loss: 2.0880 - categorical_accuracy: 0.3883 - val_loss: 2.0146 - val_categorical_accuracy: 0.4214\n",
            "Epoch 10/70\n",
            "87640/87640 [==============================] - 5s 57us/step - loss: 1.9943 - categorical_accuracy: 0.4141 - val_loss: 1.9413 - val_categorical_accuracy: 0.4376\n",
            "Epoch 11/70\n",
            "87640/87640 [==============================] - 5s 57us/step - loss: 1.9381 - categorical_accuracy: 0.4291 - val_loss: 1.9050 - val_categorical_accuracy: 0.4508\n",
            "Epoch 12/70\n",
            "87640/87640 [==============================] - 5s 57us/step - loss: 1.8687 - categorical_accuracy: 0.4478 - val_loss: 1.8443 - val_categorical_accuracy: 0.4652\n",
            "Epoch 13/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.8199 - categorical_accuracy: 0.4596 - val_loss: 1.8239 - val_categorical_accuracy: 0.4719\n",
            "Epoch 14/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.7693 - categorical_accuracy: 0.4750 - val_loss: 1.7958 - val_categorical_accuracy: 0.4755\n",
            "Epoch 15/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.7350 - categorical_accuracy: 0.4807 - val_loss: 1.7877 - val_categorical_accuracy: 0.4806\n",
            "Epoch 16/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.6938 - categorical_accuracy: 0.4922 - val_loss: 1.7381 - val_categorical_accuracy: 0.4883\n",
            "Epoch 17/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.6755 - categorical_accuracy: 0.4954 - val_loss: 1.7442 - val_categorical_accuracy: 0.4841\n",
            "Epoch 18/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.6379 - categorical_accuracy: 0.5053 - val_loss: 1.6952 - val_categorical_accuracy: 0.4956\n",
            "Epoch 19/70\n",
            "87640/87640 [==============================] - 5s 58us/step - loss: 1.6087 - categorical_accuracy: 0.5151 - val_loss: 1.7004 - val_categorical_accuracy: 0.4949\n",
            "Epoch 20/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.5904 - categorical_accuracy: 0.5187 - val_loss: 1.6711 - val_categorical_accuracy: 0.5034\n",
            "Epoch 21/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.5712 - categorical_accuracy: 0.5214 - val_loss: 1.6658 - val_categorical_accuracy: 0.5072\n",
            "Epoch 22/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.5495 - categorical_accuracy: 0.5283 - val_loss: 1.6373 - val_categorical_accuracy: 0.5093\n",
            "Epoch 23/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.5315 - categorical_accuracy: 0.5325 - val_loss: 1.6454 - val_categorical_accuracy: 0.5119\n",
            "Epoch 24/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.5205 - categorical_accuracy: 0.5348 - val_loss: 1.6536 - val_categorical_accuracy: 0.5086\n",
            "Epoch 25/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4920 - categorical_accuracy: 0.5425 - val_loss: 1.6640 - val_categorical_accuracy: 0.5055\n",
            "Epoch 26/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4757 - categorical_accuracy: 0.5456 - val_loss: 1.6019 - val_categorical_accuracy: 0.5236\n",
            "Epoch 27/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4655 - categorical_accuracy: 0.5496 - val_loss: 1.6155 - val_categorical_accuracy: 0.5181\n",
            "Epoch 28/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4473 - categorical_accuracy: 0.5537 - val_loss: 1.5930 - val_categorical_accuracy: 0.5250\n",
            "Epoch 29/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4216 - categorical_accuracy: 0.5603 - val_loss: 1.5812 - val_categorical_accuracy: 0.5302\n",
            "Epoch 30/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4183 - categorical_accuracy: 0.5599 - val_loss: 1.5799 - val_categorical_accuracy: 0.5264\n",
            "Epoch 31/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.4100 - categorical_accuracy: 0.5622 - val_loss: 1.5723 - val_categorical_accuracy: 0.5299\n",
            "Epoch 32/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.3955 - categorical_accuracy: 0.5668 - val_loss: 1.5592 - val_categorical_accuracy: 0.5333\n",
            "Epoch 33/70\n",
            "87640/87640 [==============================] - 5s 59us/step - loss: 1.3820 - categorical_accuracy: 0.5703 - val_loss: 1.5558 - val_categorical_accuracy: 0.5397\n",
            "Epoch 34/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3706 - categorical_accuracy: 0.5724 - val_loss: 1.6157 - val_categorical_accuracy: 0.5208\n",
            "Epoch 35/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3850 - categorical_accuracy: 0.5684 - val_loss: 1.5447 - val_categorical_accuracy: 0.5383\n",
            "Epoch 36/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3465 - categorical_accuracy: 0.5773 - val_loss: 1.5467 - val_categorical_accuracy: 0.5382\n",
            "Epoch 37/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3354 - categorical_accuracy: 0.5790 - val_loss: 1.5317 - val_categorical_accuracy: 0.5463\n",
            "Epoch 38/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3219 - categorical_accuracy: 0.5845 - val_loss: 1.5332 - val_categorical_accuracy: 0.5420\n",
            "Epoch 39/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3299 - categorical_accuracy: 0.5826 - val_loss: 1.5280 - val_categorical_accuracy: 0.5436\n",
            "Epoch 40/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3066 - categorical_accuracy: 0.5894 - val_loss: 1.5219 - val_categorical_accuracy: 0.5494\n",
            "Epoch 41/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.3019 - categorical_accuracy: 0.5909 - val_loss: 1.5195 - val_categorical_accuracy: 0.5490\n",
            "Epoch 42/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2876 - categorical_accuracy: 0.5952 - val_loss: 1.5126 - val_categorical_accuracy: 0.5517\n",
            "Epoch 43/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2916 - categorical_accuracy: 0.5925 - val_loss: 1.5125 - val_categorical_accuracy: 0.5503\n",
            "Epoch 44/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2838 - categorical_accuracy: 0.5926 - val_loss: 1.5216 - val_categorical_accuracy: 0.5503\n",
            "Epoch 45/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2662 - categorical_accuracy: 0.5996 - val_loss: 1.5071 - val_categorical_accuracy: 0.5520\n",
            "Epoch 46/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.2578 - categorical_accuracy: 0.6010 - val_loss: 1.5205 - val_categorical_accuracy: 0.5533\n",
            "Epoch 47/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.2662 - categorical_accuracy: 0.5989 - val_loss: 1.5140 - val_categorical_accuracy: 0.5566\n",
            "Epoch 48/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.2506 - categorical_accuracy: 0.6031 - val_loss: 1.5027 - val_categorical_accuracy: 0.5574\n",
            "Epoch 49/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2394 - categorical_accuracy: 0.6042 - val_loss: 1.4962 - val_categorical_accuracy: 0.5607\n",
            "Epoch 50/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2359 - categorical_accuracy: 0.6074 - val_loss: 1.5020 - val_categorical_accuracy: 0.5537\n",
            "Epoch 51/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.2280 - categorical_accuracy: 0.6088 - val_loss: 1.5040 - val_categorical_accuracy: 0.5541\n",
            "Epoch 52/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.2380 - categorical_accuracy: 0.6023 - val_loss: 1.5294 - val_categorical_accuracy: 0.5521\n",
            "Epoch 53/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2105 - categorical_accuracy: 0.6143 - val_loss: 1.4948 - val_categorical_accuracy: 0.5580\n",
            "Epoch 54/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2126 - categorical_accuracy: 0.6115 - val_loss: 1.4887 - val_categorical_accuracy: 0.5588\n",
            "Epoch 55/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.2023 - categorical_accuracy: 0.6143 - val_loss: 1.4907 - val_categorical_accuracy: 0.5604\n",
            "Epoch 56/70\n",
            "87640/87640 [==============================] - 5s 60us/step - loss: 1.1898 - categorical_accuracy: 0.6200 - val_loss: 1.4828 - val_categorical_accuracy: 0.5636\n",
            "Epoch 57/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.2122 - categorical_accuracy: 0.6099 - val_loss: 1.4891 - val_categorical_accuracy: 0.5663\n",
            "Epoch 58/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1850 - categorical_accuracy: 0.6186 - val_loss: 1.4881 - val_categorical_accuracy: 0.5637\n",
            "Epoch 59/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1758 - categorical_accuracy: 0.6210 - val_loss: 1.4900 - val_categorical_accuracy: 0.5652\n",
            "Epoch 60/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1861 - categorical_accuracy: 0.6187 - val_loss: 1.5126 - val_categorical_accuracy: 0.5563\n",
            "Epoch 61/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1684 - categorical_accuracy: 0.6234 - val_loss: 1.4949 - val_categorical_accuracy: 0.5612\n",
            "Epoch 62/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1687 - categorical_accuracy: 0.6224 - val_loss: 1.4802 - val_categorical_accuracy: 0.5693\n",
            "Epoch 63/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1688 - categorical_accuracy: 0.6231 - val_loss: 1.4797 - val_categorical_accuracy: 0.5651\n",
            "Epoch 64/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1578 - categorical_accuracy: 0.6258 - val_loss: 1.4756 - val_categorical_accuracy: 0.5705\n",
            "Epoch 65/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1684 - categorical_accuracy: 0.6211 - val_loss: 1.4857 - val_categorical_accuracy: 0.5678\n",
            "Epoch 66/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1472 - categorical_accuracy: 0.6282 - val_loss: 1.4761 - val_categorical_accuracy: 0.5679\n",
            "Epoch 67/70\n",
            "87640/87640 [==============================] - 5s 62us/step - loss: 1.1449 - categorical_accuracy: 0.6288 - val_loss: 1.4951 - val_categorical_accuracy: 0.5658\n",
            "Epoch 68/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1702 - categorical_accuracy: 0.6216 - val_loss: 1.4775 - val_categorical_accuracy: 0.5694\n",
            "Epoch 69/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1468 - categorical_accuracy: 0.6280 - val_loss: 1.4817 - val_categorical_accuracy: 0.5705\n",
            "Epoch 70/70\n",
            "87640/87640 [==============================] - 5s 61us/step - loss: 1.1575 - categorical_accuracy: 0.6252 - val_loss: 1.4683 - val_categorical_accuracy: 0.5710\n",
            "all r out: all r       a o\n",
            "ll ro out: ll ro       a o\n",
            "l rol out: l rol         i\n",
            " roll out:  roll         i\n",
            "rolli out: rolli         i\n",
            "Running 100000 sequences 70 epochs Hip-Hop 20 sequence length 4 layers vocab size ./gdrive/My Drive/Colab Notebooks/exps3/_slen20 directory\n",
            "X shape (96823, 20, 1)\n",
            "Train on 87140 samples, validate on 9683 samples\n",
            "Epoch 1/70\n",
            "87140/87140 [==============================] - 20s 227us/step - loss: 2.9615 - categorical_accuracy: 0.2081 - val_loss: 2.8883 - val_categorical_accuracy: 0.2295\n",
            "Epoch 2/70\n",
            "87140/87140 [==============================] - 19s 213us/step - loss: 2.8941 - categorical_accuracy: 0.2206 - val_loss: 2.8924 - val_categorical_accuracy: 0.2295\n",
            "Epoch 3/70\n",
            "87140/87140 [==============================] - 19s 215us/step - loss: 2.8901 - categorical_accuracy: 0.2206 - val_loss: 2.8890 - val_categorical_accuracy: 0.2295\n",
            "Epoch 4/70\n",
            "87140/87140 [==============================] - 19s 216us/step - loss: 2.8883 - categorical_accuracy: 0.2206 - val_loss: 2.8918 - val_categorical_accuracy: 0.2295\n",
            "Epoch 5/70\n",
            "87140/87140 [==============================] - 19s 217us/step - loss: 2.8854 - categorical_accuracy: 0.2206 - val_loss: 2.8725 - val_categorical_accuracy: 0.2295\n",
            "Epoch 6/70\n",
            "87140/87140 [==============================] - 19s 218us/step - loss: 2.7576 - categorical_accuracy: 0.2366 - val_loss: 2.6164 - val_categorical_accuracy: 0.2775\n",
            "Epoch 7/70\n",
            "87140/87140 [==============================] - 19s 219us/step - loss: 2.5610 - categorical_accuracy: 0.2762 - val_loss: 2.4946 - val_categorical_accuracy: 0.2969\n",
            "Epoch 8/70\n",
            "87140/87140 [==============================] - 19s 220us/step - loss: 2.4153 - categorical_accuracy: 0.3051 - val_loss: 2.3330 - val_categorical_accuracy: 0.3392\n",
            "Epoch 9/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 2.2597 - categorical_accuracy: 0.3467 - val_loss: 2.1846 - val_categorical_accuracy: 0.3736\n",
            "Epoch 10/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 2.1331 - categorical_accuracy: 0.3780 - val_loss: 2.0688 - val_categorical_accuracy: 0.4025\n",
            "Epoch 11/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 2.0172 - categorical_accuracy: 0.4123 - val_loss: 2.0312 - val_categorical_accuracy: 0.4095\n",
            "Epoch 12/70\n",
            "87140/87140 [==============================] - 19s 220us/step - loss: 1.9057 - categorical_accuracy: 0.4443 - val_loss: 1.9195 - val_categorical_accuracy: 0.4421\n",
            "Epoch 13/70\n",
            "87140/87140 [==============================] - 19s 220us/step - loss: 1.8063 - categorical_accuracy: 0.4723 - val_loss: 1.8494 - val_categorical_accuracy: 0.4646\n",
            "Epoch 14/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 1.7169 - categorical_accuracy: 0.4973 - val_loss: 1.8323 - val_categorical_accuracy: 0.4705\n",
            "Epoch 15/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 1.6315 - categorical_accuracy: 0.5233 - val_loss: 1.7838 - val_categorical_accuracy: 0.4815\n",
            "Epoch 16/70\n",
            "87140/87140 [==============================] - 19s 220us/step - loss: 1.5472 - categorical_accuracy: 0.5472 - val_loss: 1.7391 - val_categorical_accuracy: 0.4936\n",
            "Epoch 17/70\n",
            "87140/87140 [==============================] - 19s 220us/step - loss: 1.4681 - categorical_accuracy: 0.5696 - val_loss: 1.7099 - val_categorical_accuracy: 0.4995\n",
            "Epoch 18/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 1.3912 - categorical_accuracy: 0.5927 - val_loss: 1.6949 - val_categorical_accuracy: 0.5080\n",
            "Epoch 19/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 1.3146 - categorical_accuracy: 0.6143 - val_loss: 1.6537 - val_categorical_accuracy: 0.5240\n",
            "Epoch 20/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 1.2421 - categorical_accuracy: 0.6342 - val_loss: 1.6108 - val_categorical_accuracy: 0.5363\n",
            "Epoch 21/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 1.1709 - categorical_accuracy: 0.6553 - val_loss: 1.5904 - val_categorical_accuracy: 0.5434\n",
            "Epoch 22/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 1.1037 - categorical_accuracy: 0.6748 - val_loss: 1.5606 - val_categorical_accuracy: 0.5598\n",
            "Epoch 23/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 1.0363 - categorical_accuracy: 0.6919 - val_loss: 1.5975 - val_categorical_accuracy: 0.5517\n",
            "Epoch 24/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.9777 - categorical_accuracy: 0.7111 - val_loss: 1.5318 - val_categorical_accuracy: 0.5792\n",
            "Epoch 25/70\n",
            "87140/87140 [==============================] - 19s 220us/step - loss: 0.9169 - categorical_accuracy: 0.7265 - val_loss: 1.5419 - val_categorical_accuracy: 0.5838\n",
            "Epoch 26/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.8524 - categorical_accuracy: 0.7442 - val_loss: 1.5411 - val_categorical_accuracy: 0.5835\n",
            "Epoch 27/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.8097 - categorical_accuracy: 0.7571 - val_loss: 1.5059 - val_categorical_accuracy: 0.6013\n",
            "Epoch 28/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.7504 - categorical_accuracy: 0.7741 - val_loss: 1.4989 - val_categorical_accuracy: 0.6097\n",
            "Epoch 29/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.6961 - categorical_accuracy: 0.7884 - val_loss: 1.4929 - val_categorical_accuracy: 0.6208\n",
            "Epoch 30/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.6511 - categorical_accuracy: 0.8006 - val_loss: 1.5212 - val_categorical_accuracy: 0.6209\n",
            "Epoch 31/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.6142 - categorical_accuracy: 0.8129 - val_loss: 1.5251 - val_categorical_accuracy: 0.6316\n",
            "Epoch 32/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.5754 - categorical_accuracy: 0.8226 - val_loss: 1.5530 - val_categorical_accuracy: 0.6273\n",
            "Epoch 33/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.5368 - categorical_accuracy: 0.8333 - val_loss: 1.5291 - val_categorical_accuracy: 0.6386\n",
            "Epoch 34/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.4974 - categorical_accuracy: 0.8458 - val_loss: 1.5507 - val_categorical_accuracy: 0.6445\n",
            "Epoch 35/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.4646 - categorical_accuracy: 0.8559 - val_loss: 1.5373 - val_categorical_accuracy: 0.6517\n",
            "Epoch 36/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.4314 - categorical_accuracy: 0.8666 - val_loss: 1.5708 - val_categorical_accuracy: 0.6516\n",
            "Epoch 37/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.4126 - categorical_accuracy: 0.8720 - val_loss: 1.5411 - val_categorical_accuracy: 0.6624\n",
            "Epoch 38/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.3853 - categorical_accuracy: 0.8794 - val_loss: 1.5531 - val_categorical_accuracy: 0.6651\n",
            "Epoch 39/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.3571 - categorical_accuracy: 0.8887 - val_loss: 1.6262 - val_categorical_accuracy: 0.6554\n",
            "Epoch 40/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.3393 - categorical_accuracy: 0.8947 - val_loss: 1.6699 - val_categorical_accuracy: 0.6507\n",
            "Epoch 41/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.3231 - categorical_accuracy: 0.8994 - val_loss: 1.6350 - val_categorical_accuracy: 0.6663\n",
            "Epoch 42/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.3013 - categorical_accuracy: 0.9070 - val_loss: 1.6733 - val_categorical_accuracy: 0.6656\n",
            "Epoch 43/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.2815 - categorical_accuracy: 0.9134 - val_loss: 1.6792 - val_categorical_accuracy: 0.6642\n",
            "Epoch 44/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.2671 - categorical_accuracy: 0.9167 - val_loss: 1.7120 - val_categorical_accuracy: 0.6583\n",
            "Epoch 45/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.2639 - categorical_accuracy: 0.9173 - val_loss: 1.7290 - val_categorical_accuracy: 0.6697\n",
            "Epoch 46/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.2614 - categorical_accuracy: 0.9184 - val_loss: 1.7196 - val_categorical_accuracy: 0.6707\n",
            "Epoch 47/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.2422 - categorical_accuracy: 0.9253 - val_loss: 1.7473 - val_categorical_accuracy: 0.6685\n",
            "Epoch 48/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.2307 - categorical_accuracy: 0.9276 - val_loss: 1.7579 - val_categorical_accuracy: 0.6684\n",
            "Epoch 49/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.2097 - categorical_accuracy: 0.9349 - val_loss: 1.7789 - val_categorical_accuracy: 0.6730\n",
            "Epoch 50/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.2066 - categorical_accuracy: 0.9365 - val_loss: 1.8025 - val_categorical_accuracy: 0.6665\n",
            "Epoch 51/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.2070 - categorical_accuracy: 0.9367 - val_loss: 1.8182 - val_categorical_accuracy: 0.6668\n",
            "Epoch 52/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1888 - categorical_accuracy: 0.9423 - val_loss: 1.8339 - val_categorical_accuracy: 0.6720\n",
            "Epoch 53/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1842 - categorical_accuracy: 0.9427 - val_loss: 1.8485 - val_categorical_accuracy: 0.6694\n",
            "Epoch 54/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1782 - categorical_accuracy: 0.9452 - val_loss: 1.8852 - val_categorical_accuracy: 0.6668\n",
            "Epoch 55/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1724 - categorical_accuracy: 0.9479 - val_loss: 1.8849 - val_categorical_accuracy: 0.6691\n",
            "Epoch 56/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1784 - categorical_accuracy: 0.9448 - val_loss: 1.9329 - val_categorical_accuracy: 0.6662\n",
            "Epoch 57/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.2555 - categorical_accuracy: 0.9189 - val_loss: 1.9134 - val_categorical_accuracy: 0.6679\n",
            "Epoch 58/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1646 - categorical_accuracy: 0.9484 - val_loss: 1.9297 - val_categorical_accuracy: 0.6651\n",
            "Epoch 59/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1507 - categorical_accuracy: 0.9543 - val_loss: 1.9687 - val_categorical_accuracy: 0.6686\n",
            "Epoch 60/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1447 - categorical_accuracy: 0.9558 - val_loss: 1.9778 - val_categorical_accuracy: 0.6655\n",
            "Epoch 61/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1398 - categorical_accuracy: 0.9572 - val_loss: 1.9531 - val_categorical_accuracy: 0.6714\n",
            "Epoch 62/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1386 - categorical_accuracy: 0.9571 - val_loss: 1.9793 - val_categorical_accuracy: 0.6706\n",
            "Epoch 63/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1431 - categorical_accuracy: 0.9563 - val_loss: 1.9851 - val_categorical_accuracy: 0.6661\n",
            "Epoch 64/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1354 - categorical_accuracy: 0.9581 - val_loss: 2.0184 - val_categorical_accuracy: 0.6593\n",
            "Epoch 65/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1326 - categorical_accuracy: 0.9597 - val_loss: 2.0143 - val_categorical_accuracy: 0.6659\n",
            "Epoch 66/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1309 - categorical_accuracy: 0.9600 - val_loss: 2.0524 - val_categorical_accuracy: 0.6659\n",
            "Epoch 67/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1309 - categorical_accuracy: 0.9591 - val_loss: 2.0253 - val_categorical_accuracy: 0.6706\n",
            "Epoch 68/70\n",
            "87140/87140 [==============================] - 19s 221us/step - loss: 0.1302 - categorical_accuracy: 0.9598 - val_loss: 2.0332 - val_categorical_accuracy: 0.6696\n",
            "Epoch 69/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1261 - categorical_accuracy: 0.9611 - val_loss: 2.1000 - val_categorical_accuracy: 0.6675\n",
            "Epoch 70/70\n",
            "87140/87140 [==============================] - 19s 222us/step - loss: 0.1302 - categorical_accuracy: 0.9591 - val_loss: 2.1013 - val_categorical_accuracy: 0.6667\n",
            " and got the 8ball r out:  and got the 8ball r o N o N N 2 N o N 2 r N N N N N 2 N o 2\n",
            "and got the 8ball ro out: and got the 8ball ro N o N N 2 N o N 2 r N N N N N 2 N o 2 N\n",
            "nd got the 8ball rol out: nd got the 8ball rol o N N 2 N o N 2 r 2 N N 2 2 2 2 N N N N\n",
            "d got the 8ball roll out: d got the 8ball roll N N 2 N o N 2 r 2 N N 2 2 2 2 N N y N N\n",
            " got the 8ball rolli out:  got the 8ball rolli N 2 2 N N N N N N N N N   N o N N a N N\n",
            "Running 100000 sequences 70 epochs Hip-Hop 30 sequence length 4 layers vocab size ./gdrive/My Drive/Colab Notebooks/exps3/_slen30 directory\n",
            "X shape (96453, 30, 1)\n",
            "Train on 86807 samples, validate on 9646 samples\n",
            "Epoch 1/70\n",
            "86807/86807 [==============================] - 29s 334us/step - loss: 2.9394 - categorical_accuracy: 0.2141 - val_loss: 2.8904 - val_categorical_accuracy: 0.2295\n",
            "Epoch 2/70\n",
            "86807/86807 [==============================] - 28s 319us/step - loss: 2.8940 - categorical_accuracy: 0.2206 - val_loss: 2.8907 - val_categorical_accuracy: 0.2295\n",
            "Epoch 3/70\n",
            "86807/86807 [==============================] - 28s 322us/step - loss: 2.8907 - categorical_accuracy: 0.2206 - val_loss: 2.8858 - val_categorical_accuracy: 0.2295\n",
            "Epoch 4/70\n",
            "86807/86807 [==============================] - 28s 325us/step - loss: 2.8880 - categorical_accuracy: 0.2206 - val_loss: 2.8876 - val_categorical_accuracy: 0.2295\n",
            "Epoch 5/70\n",
            "86807/86807 [==============================] - 28s 327us/step - loss: 2.8504 - categorical_accuracy: 0.2209 - val_loss: 2.7289 - val_categorical_accuracy: 0.2365\n",
            "Epoch 6/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 2.6258 - categorical_accuracy: 0.2641 - val_loss: 2.5334 - val_categorical_accuracy: 0.2951\n",
            "Epoch 7/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 2.4861 - categorical_accuracy: 0.2904 - val_loss: 2.3751 - val_categorical_accuracy: 0.3219\n",
            "Epoch 8/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 2.3179 - categorical_accuracy: 0.3297 - val_loss: 2.2145 - val_categorical_accuracy: 0.3710\n",
            "Epoch 9/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 2.1583 - categorical_accuracy: 0.3720 - val_loss: 2.0718 - val_categorical_accuracy: 0.4044\n",
            "Epoch 10/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 2.0297 - categorical_accuracy: 0.4072 - val_loss: 1.9942 - val_categorical_accuracy: 0.4217\n",
            "Epoch 11/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.9209 - categorical_accuracy: 0.4407 - val_loss: 1.9174 - val_categorical_accuracy: 0.4417\n",
            "Epoch 12/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 1.8262 - categorical_accuracy: 0.4665 - val_loss: 1.8867 - val_categorical_accuracy: 0.4535\n",
            "Epoch 13/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.7518 - categorical_accuracy: 0.4890 - val_loss: 1.7990 - val_categorical_accuracy: 0.4777\n",
            "Epoch 14/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.6585 - categorical_accuracy: 0.5170 - val_loss: 1.7708 - val_categorical_accuracy: 0.4835\n",
            "Epoch 15/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.5843 - categorical_accuracy: 0.5361 - val_loss: 1.7203 - val_categorical_accuracy: 0.4983\n",
            "Epoch 16/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.5113 - categorical_accuracy: 0.5552 - val_loss: 1.7057 - val_categorical_accuracy: 0.5041\n",
            "Epoch 17/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.4495 - categorical_accuracy: 0.5733 - val_loss: 1.6947 - val_categorical_accuracy: 0.5044\n",
            "Epoch 18/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.3761 - categorical_accuracy: 0.5944 - val_loss: 1.6492 - val_categorical_accuracy: 0.5211\n",
            "Epoch 19/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.3127 - categorical_accuracy: 0.6112 - val_loss: 1.6092 - val_categorical_accuracy: 0.5299\n",
            "Epoch 20/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 1.2519 - categorical_accuracy: 0.6311 - val_loss: 1.6040 - val_categorical_accuracy: 0.5365\n",
            "Epoch 21/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 1.1866 - categorical_accuracy: 0.6504 - val_loss: 1.5803 - val_categorical_accuracy: 0.5426\n",
            "Epoch 22/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 1.1277 - categorical_accuracy: 0.6670 - val_loss: 1.5454 - val_categorical_accuracy: 0.5584\n",
            "Epoch 23/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 1.0706 - categorical_accuracy: 0.6817 - val_loss: 1.5314 - val_categorical_accuracy: 0.5628\n",
            "Epoch 24/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 1.0196 - categorical_accuracy: 0.6966 - val_loss: 1.5222 - val_categorical_accuracy: 0.5694\n",
            "Epoch 25/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.9643 - categorical_accuracy: 0.7118 - val_loss: 1.5050 - val_categorical_accuracy: 0.5793\n",
            "Epoch 26/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.9116 - categorical_accuracy: 0.7271 - val_loss: 1.4893 - val_categorical_accuracy: 0.5866\n",
            "Epoch 27/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.8616 - categorical_accuracy: 0.7422 - val_loss: 1.4929 - val_categorical_accuracy: 0.5960\n",
            "Epoch 28/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.8186 - categorical_accuracy: 0.7540 - val_loss: 1.4767 - val_categorical_accuracy: 0.6023\n",
            "Epoch 29/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.7737 - categorical_accuracy: 0.7664 - val_loss: 1.4730 - val_categorical_accuracy: 0.6100\n",
            "Epoch 30/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.7249 - categorical_accuracy: 0.7808 - val_loss: 1.4826 - val_categorical_accuracy: 0.6102\n",
            "Epoch 31/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.6904 - categorical_accuracy: 0.7890 - val_loss: 1.4548 - val_categorical_accuracy: 0.6223\n",
            "Epoch 32/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.6493 - categorical_accuracy: 0.8020 - val_loss: 1.4716 - val_categorical_accuracy: 0.6264\n",
            "Epoch 33/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.6127 - categorical_accuracy: 0.8119 - val_loss: 1.5048 - val_categorical_accuracy: 0.6192\n",
            "Epoch 34/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.5841 - categorical_accuracy: 0.8215 - val_loss: 1.5000 - val_categorical_accuracy: 0.6304\n",
            "Epoch 35/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.5527 - categorical_accuracy: 0.8315 - val_loss: 1.4847 - val_categorical_accuracy: 0.6433\n",
            "Epoch 36/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.5161 - categorical_accuracy: 0.8410 - val_loss: 1.5037 - val_categorical_accuracy: 0.6386\n",
            "Epoch 37/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.4929 - categorical_accuracy: 0.8469 - val_loss: 1.4879 - val_categorical_accuracy: 0.6531\n",
            "Epoch 38/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.4637 - categorical_accuracy: 0.8561 - val_loss: 1.4931 - val_categorical_accuracy: 0.6522\n",
            "Epoch 39/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.4378 - categorical_accuracy: 0.8646 - val_loss: 1.5364 - val_categorical_accuracy: 0.6532\n",
            "Epoch 40/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.4179 - categorical_accuracy: 0.8699 - val_loss: 1.5216 - val_categorical_accuracy: 0.6592\n",
            "Epoch 41/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.4466 - categorical_accuracy: 0.8604 - val_loss: 1.7059 - val_categorical_accuracy: 0.6126\n",
            "Epoch 42/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.4018 - categorical_accuracy: 0.8749 - val_loss: 1.5387 - val_categorical_accuracy: 0.6654\n",
            "Epoch 43/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.3520 - categorical_accuracy: 0.8917 - val_loss: 1.5600 - val_categorical_accuracy: 0.6695\n",
            "Epoch 44/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.3332 - categorical_accuracy: 0.8966 - val_loss: 1.5774 - val_categorical_accuracy: 0.6714\n",
            "Epoch 45/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.3180 - categorical_accuracy: 0.9011 - val_loss: 1.5953 - val_categorical_accuracy: 0.6677\n",
            "Epoch 46/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.2966 - categorical_accuracy: 0.9085 - val_loss: 1.6084 - val_categorical_accuracy: 0.6696\n",
            "Epoch 47/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.2882 - categorical_accuracy: 0.9093 - val_loss: 1.6204 - val_categorical_accuracy: 0.6715\n",
            "Epoch 48/70\n",
            "86807/86807 [==============================] - 29s 329us/step - loss: 0.2744 - categorical_accuracy: 0.9153 - val_loss: 1.6397 - val_categorical_accuracy: 0.6710\n",
            "Epoch 49/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.2625 - categorical_accuracy: 0.9184 - val_loss: 1.6840 - val_categorical_accuracy: 0.6707\n",
            "Epoch 50/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.2531 - categorical_accuracy: 0.9205 - val_loss: 1.7056 - val_categorical_accuracy: 0.6718\n",
            "Epoch 51/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.2415 - categorical_accuracy: 0.9250 - val_loss: 1.6996 - val_categorical_accuracy: 0.6722\n",
            "Epoch 52/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.2346 - categorical_accuracy: 0.9271 - val_loss: 1.7003 - val_categorical_accuracy: 0.6767\n",
            "Epoch 53/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.2264 - categorical_accuracy: 0.9295 - val_loss: 1.7338 - val_categorical_accuracy: 0.6715\n",
            "Epoch 54/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.2173 - categorical_accuracy: 0.9320 - val_loss: 1.7651 - val_categorical_accuracy: 0.6672\n",
            "Epoch 55/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.2089 - categorical_accuracy: 0.9353 - val_loss: 1.7742 - val_categorical_accuracy: 0.6716\n",
            "Epoch 56/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.2014 - categorical_accuracy: 0.9379 - val_loss: 1.8005 - val_categorical_accuracy: 0.6730\n",
            "Epoch 57/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.2007 - categorical_accuracy: 0.9380 - val_loss: 1.7943 - val_categorical_accuracy: 0.6755\n",
            "Epoch 58/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.1921 - categorical_accuracy: 0.9393 - val_loss: 1.8202 - val_categorical_accuracy: 0.6694\n",
            "Epoch 59/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.1863 - categorical_accuracy: 0.9418 - val_loss: 1.8067 - val_categorical_accuracy: 0.6708\n",
            "Epoch 60/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.1818 - categorical_accuracy: 0.9431 - val_loss: 1.8343 - val_categorical_accuracy: 0.6706\n",
            "Epoch 61/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.1849 - categorical_accuracy: 0.9415 - val_loss: 1.8682 - val_categorical_accuracy: 0.6676\n",
            "Epoch 62/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.1688 - categorical_accuracy: 0.9485 - val_loss: 1.8752 - val_categorical_accuracy: 0.6701\n",
            "Epoch 63/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.1637 - categorical_accuracy: 0.9490 - val_loss: 1.8976 - val_categorical_accuracy: 0.6714\n",
            "Epoch 64/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.1581 - categorical_accuracy: 0.9506 - val_loss: 1.8883 - val_categorical_accuracy: 0.6718\n",
            "Epoch 65/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.1547 - categorical_accuracy: 0.9517 - val_loss: 1.9071 - val_categorical_accuracy: 0.6682\n",
            "Epoch 66/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.1551 - categorical_accuracy: 0.9518 - val_loss: 1.9205 - val_categorical_accuracy: 0.6710\n",
            "Epoch 67/70\n",
            "86807/86807 [==============================] - 29s 333us/step - loss: 0.1531 - categorical_accuracy: 0.9524 - val_loss: 1.9115 - val_categorical_accuracy: 0.6740\n",
            "Epoch 68/70\n",
            "86807/86807 [==============================] - 29s 332us/step - loss: 0.1500 - categorical_accuracy: 0.9526 - val_loss: 1.9422 - val_categorical_accuracy: 0.6707\n",
            "Epoch 69/70\n",
            "86807/86807 [==============================] - 29s 331us/step - loss: 0.1445 - categorical_accuracy: 0.9546 - val_loss: 1.9371 - val_categorical_accuracy: 0.6741\n",
            "Epoch 70/70\n",
            "86807/86807 [==============================] - 29s 330us/step - loss: 0.1511 - categorical_accuracy: 0.9522 - val_loss: 1.9420 - val_categorical_accuracy: 0.6728\n",
            "fcked up N and got the 8ball r out: fcked up N and got the 8ball r N 9 9   N i N   N N N   N N i N   N i i 7 8   N N N N 8 8 8\n",
            "cked up N and got the 8ball ro out: cked up N and got the 8ball ro 9 9   N i N   N N   N N   N 4 N 4 N N   N e i N   N N N 7  \n",
            "ked up N and got the 8ball rol out: ked up N and got the 8ball rol 9   N i N   N N e N   N N i N   N i i i   N i i N 8   N N i\n",
            "ed up N and got the 8ball roll out: ed up N and got the 8ball roll   N i N   N N   N N   N 4 N 4 N N   N e i 8   N N N 7 N   N\n",
            "d up N and got the 8ball rolli out: d up N and got the 8ball rolli   N N   N N   N 4 N 8   N   N e i   N 8 i   N i   N N i   N\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKg8dHxF4Lkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}